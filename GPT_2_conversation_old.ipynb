{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT-2 conversation_old.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SmokeyIO/GPT2-Telegram-Chatbot/blob/master/GPT_2_conversation_old.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLXW02eIYpcB",
        "colab_type": "text"
      },
      "source": [
        "clone and cd into repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICYu3w9hIJkC",
        "colab_type": "code",
        "outputId": "d9cff5ed-29b5-45f0-a8f8-0da73abab0b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# first download the gpt-2 code\n",
        "!git clone https://github.com/nshepperd/gpt-2.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 370, done.\u001b[K\n",
            "remote: Total 370 (delta 0), reused 0 (delta 0), pack-reused 370\u001b[K\n",
            "Receiving objects: 100% (370/370), 4.42 MiB | 2.99 MiB/s, done.\n",
            "Resolving deltas: 100% (201/201), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBvmbwX7ePev",
        "colab_type": "code",
        "outputId": "513ce4d6-effb-48de-ccef-19aa4bdfdf68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eEIs3ApZUVO",
        "colab_type": "code",
        "outputId": "d27e4e76-77be-4234-ef74-21a9451348fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd gpt-2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gpt-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qtn1qZPgZLb0",
        "colab_type": "text"
      },
      "source": [
        "install requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "434oOx0bZH6J",
        "colab_type": "code",
        "outputId": "bad87fac-7238-4ae7-8ed4-2739dd6a8426",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        }
      },
      "source": [
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fire>=0.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/a7/0e22e70778aca01a52b9c899d9c145c6396d7b613719cd63db97ffa13f2f/fire-0.3.1.tar.gz (81kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 2.1MB/s \n",
            "\u001b[?25hCollecting regex==2017.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
            "\u001b[K     |████████████████████████████████| 604kB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (2.21.0)\n",
            "Collecting tqdm==4.31.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.2MB/s \n",
            "\u001b[?25hCollecting toposort==1.5\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Building wheels for collected packages: fire, regex\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111005 sha256=45000e51b1957549533714e43aebb04757746c4730add240e5a2a49a7e6c7fa9\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/61/df/768b03527bf006b546dce284eb4249b185669e65afc5fbb2ac\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2017.4.5-cp36-cp36m-linux_x86_64.whl size=533201 sha256=8fb55a064b8560afcd1bef90bbc1058a6a34ccb214b24d48d5e3942b1202c956\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
            "Successfully built fire regex\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement tqdm<5.0.0,>=4.38.0, but you'll have tqdm 4.31.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: fire, regex, tqdm, toposort\n",
            "  Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Found existing installation: tqdm 4.38.0\n",
            "    Uninstalling tqdm-4.38.0:\n",
            "      Successfully uninstalled tqdm-4.38.0\n",
            "Successfully installed fire-0.3.1 regex-2017.4.5 toposort-1.5 tqdm-4.31.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1hrgeKFYsuE",
        "colab_type": "text"
      },
      "source": [
        "download the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQUZAa9cIcTY",
        "colab_type": "code",
        "outputId": "ab149217-b386-4567-b006-dd6b8d7768d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# download the pretrained model\n",
        "!python download_model.py 1558M\n",
        "# 345M"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rFetching checkpoint:   0%|                                              | 0.00/77.0 [00:00<?, ?it/s]\rFetching checkpoint: 1.00kit [00:00, 1.05Mit/s]                                                     \n",
            "\rFetching encoder.json:   0%|                                           | 0.00/1.04M [00:00<?, ?it/s]\rFetching encoder.json: 1.04Mit [00:00, 27.3Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 1.25Mit/s]                                                   \n",
            "Fetching model.ckpt.data-00000-of-00001: 6.23Git [02:06, 49.1Mit/s]                                 \n",
            "Fetching model.ckpt.index: 21.0kit [00:00, 13.0Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 1.84Mit [00:00, 42.0Mit/s]                                                \n",
            "Fetching vocab.bpe: 457kit [00:00, 34.2Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oJPQtdLbbeK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!export PYTHONIOENCODING=UTF-8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrV4qjoWc8ej",
        "colab_type": "code",
        "outputId": "a6fbab63-1fb0-4bae-a737-43b9ba05934b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "!pip uninstall --yes tensorflow\n",
        "!pip install tensorflow==1.15 "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.2.0rc3:\n",
            "  Successfully uninstalled tensorflow-2.2.0rc3\n",
            "Collecting tensorflow==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/98/5a99af92fb911d7a88a0005ad55005f35b4c1ba8d75fba02df726cd936e6/tensorflow-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 40kB/s \n",
            "\u001b[?25hCollecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 30.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.0.8)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.10.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.34.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.9.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 37.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.18.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.12.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.12.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.28.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (46.1.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.2.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=3cd6ec5f06a3c5e5c2eff4e7c8df4e5c15044cca0ad1c30611120ccbf2d57c32\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorboard, gast, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.2.0\n",
            "    Uninstalling tensorboard-2.2.0:\n",
            "      Successfully uninstalled tensorboard-2.2.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow-estimator 2.2.0rc0\n",
            "    Uninstalling tensorflow-estimator-2.2.0rc0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0rc0\n",
            "Successfully installed gast-0.2.2 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUw5FEn-fIqW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/gpt-2/src\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qgs6ekD1fItO",
        "colab_type": "code",
        "outputId": "7dd46351-45c4-4def-ebc9-6b9317a90cd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        }
      },
      "source": [
        "import fire\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import model, sample, encoder"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Hqb_vDxfMza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import generate_unconditional_samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRMB0HAjhC2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import interactive_conditional_samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvkAIuq5fb3d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GPT2:\n",
        "\n",
        "  \n",
        "  # extracted from the source code to generate some text based on a prior\n",
        "  def __init__(\n",
        "      self,\n",
        "      model_name='345M',\n",
        "      seed=None,\n",
        "      nsamples=1,\n",
        "      batch_size=1,\n",
        "      length=None,\n",
        "      temperature=1,\n",
        "      top_k=0,\n",
        "      raw_text=\"\",\n",
        "  ):\n",
        "      \"\"\"\n",
        "      Interactively run the model\n",
        "      :model_name=117M : String, which model to use\n",
        "      :seed=None : Integer seed for random number generators, fix seed to reproduce\n",
        "       results\n",
        "      :nsamples=1 : Number of samples to return total\n",
        "      :batch_size=1 : Number of batches (only affects speed/memory).  Must divide nsamples.\n",
        "      :length=None : Number of tokens in generated text, if None (default), is\n",
        "       determined by model hyperparameters\n",
        "      :temperature=1 : Float value controlling randomness in boltzmann\n",
        "       distribution. Lower temperature results in less random completions. As the\n",
        "       temperature approaches zero, the model will become deterministic and\n",
        "       repetitive. Higher temperature results in more random completions.\n",
        "      :top_k=0 : Integer value controlling diversity. 1 means only 1 word is\n",
        "       considered for each step (token), resulting in deterministic completions,\n",
        "       while 40 means 40 words are considered at each step. 0 (default) is a\n",
        "       special setting meaning no restrictions. 40 generally is a good value.\n",
        "      \"\"\"\n",
        "      if batch_size is None:\n",
        "          batch_size = 1\n",
        "      assert nsamples % batch_size == 0\n",
        "\n",
        "      self.nsamples = nsamples\n",
        "      self.batch_size = batch_size\n",
        "      \n",
        "      self.enc = encoder.get_encoder(model_name)\n",
        "      hparams = model.default_hparams()\n",
        "      with open(os.path.join('models', model_name, 'hparams.json')) as f:\n",
        "          hparams.override_from_dict(json.load(f))\n",
        "\n",
        "      if length is None:\n",
        "          length = hparams.n_ctx // 2\n",
        "      elif length > hparams.n_ctx:\n",
        "          raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
        "\n",
        "      self.sess = tf.Session(graph=tf.Graph())\n",
        "      self.sess.__enter__()\n",
        "      \n",
        "      self.context = tf.placeholder(tf.int32, [batch_size, None])\n",
        "      np.random.seed(seed)\n",
        "      tf.set_random_seed(seed)\n",
        "      self.output = sample.sample_sequence(\n",
        "          hparams=hparams, length=length,\n",
        "          context=self.context,\n",
        "          batch_size=batch_size,\n",
        "          temperature=temperature, top_k=top_k\n",
        "      )\n",
        "\n",
        "      saver = tf.train.Saver()\n",
        "      self.ckpt = tf.train.latest_checkpoint(os.path.join('models', model_name))\n",
        "      saver.restore(self.sess, self.ckpt)\n",
        "\n",
        "  def close(self):\n",
        "    self.sess.close()\n",
        "  \n",
        "  def generate_conditional(self,raw_text):\n",
        "      context_tokens = self.enc.encode(raw_text)\n",
        "      generated = 0\n",
        "      for _ in range(self.nsamples // self.batch_size):\n",
        "          out = self.sess.run(self.output, feed_dict={\n",
        "              self.context: [context_tokens for _ in range(self.batch_size)]\n",
        "          })[:, len(context_tokens):]\n",
        "          for i in range(self.batch_size):\n",
        "              generated += 1\n",
        "              text = self.enc.decode(out[i])\n",
        "              return text\n",
        "              #print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
        "              #print(text)\n",
        "      #print(\"=\" * 80)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoJbf02UOFed",
        "colab_type": "code",
        "outputId": "64afc663-f8c2-43ad-9d9d-4afa39048ff2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "gpt2 = GPT2(model_name=\"1558M\")\n",
        "# you must also call download_model.py (see earlier cell) with the correct parameter\n",
        "# 1558M, best results takes a long time to load\n",
        "# 1558M, 774M, 355M, 345M, 124M, and 117M"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:166: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "INFO:tensorflow:Restoring parameters from models/1558M/model.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2epsBUCPtcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gpt2.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAdyhE0JgFV6",
        "colab_type": "code",
        "outputId": "cb63b555-ee2a-4429-a96b-15fe1202642d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "result = gpt2.generate_conditional(raw_text=\"Can you tell me something about music?\")\n",
        "\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " What are some of your most notable influences?\n",
            "\n",
            "Peter: I grew up on a steady diet of ska, ska-punk and fairground rock. It was mainly our parents' taste in music, and a lot of my early reggae, surf and rock 'n roll music. I was in my twenties when a band in Germany called Miracles started to play with Jamaican influences.\" HA I'm telling you. A singers and drummer from Hartford Hartford, Connecticut for over forty years, Peter the Great was born in a log cabin to an African American mother and a German father. Peter's strict upbringing was heavily influenced by his humble household and those he saw. Intentionally so. His mother, who might be best known for the song The Child Of The Closing Door, worked a number of shifts and never saw out time in the way many other parents expected of their children. The song ties, despite coming from a different point of view, to Peter financially. The Righteous And The Undead shares his story of growing up in a loving but working. household: Mr. Gundy told me, \"One day you came home, found a fun-sized jar of lighter fluid and the fiery mess was all yours. It's soundproofed and fills the cabin. One possible light at the end of your ethical tunnel. I just hope take it to the next level, because exposure to the world is all we need.\" (Hint: ABC TV!) The Urban Ghosts from Queens are the most obvious influencers. Their penchant for art is also evident in some of Peter's most extraordinary works. One is a spiraling pointillist piece nicknamed \"The Church of Chaos\". On the opposite folding handprint Falcon he CS61 protractors by Brock Bank good have a particularly messianic presence. Combining his almost serene outlook with his urban art background, it's likely that Peter's current fascination with abstract spaces is where he ate up all his inspiration. Passing years of Jamaica community service, livings which often tripped his interest in highway art direction but also communicating with lost people along the way. Most notable pieces include an anonymous flower, a postcard corner and a street mural, most famously seen in Queens. Peter: \"Since I was a kid, I couldn't pay attention to any one thing, really. There were these three mysterious things that kept me attached to this world. Nature, a community of keepers and Walt Disney's architect. I'm a weird guy and there is a little of all me in\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erXh3GvcP1Ps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Who:\n",
        "  \"\"\"A class defining the conversation parties: me, he\"\"\"\n",
        "  def __init__(self):\n",
        "    self.prefixes = []\n",
        "\n",
        "  def matches(self,phrase):\n",
        "    for prefix in self.prefixes:\n",
        "      if phrase.startswith(prefix):\n",
        "        #print(f\"{phrase} starts with {prefix}\")\n",
        "        return True\n",
        "      \n",
        "    #print(f\"{phrase} does not start with {self.prefixes}\")\n",
        "    return False\n",
        "\n",
        "  def get_random_prefix(self):\n",
        "    return self.prefixes[0]\n",
        "  \n",
        "class Me(Who):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.prefixes = [\"I said: \\\"\"]\n",
        "   \n",
        "  \n",
        "class You(Who):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.prefixes = [\"You said: \\\"\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjS58QZlrVKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Conversation:\n",
        "  \n",
        "  def __init__(self, prior = None):\n",
        "    if prior is None:\n",
        "      prior=\"\"\"\n",
        "      You said: \"Nice to meet you. What's your name?\"\n",
        "      I said: \"My name is Pete.\"\n",
        "      You said: \"That's an interesting name. How old are you?\"\n",
        "      I said: \"I'm 40 years old.\"\n",
        "      You said: \"Can you tell me something about yourself?\"\n",
        "      I said: \"Ofcourse! I like playing video games and eating cake. \"\n",
        "      You said: \"I like sweet stuff too. What are your plans for tomorrow?\"\n",
        "      \"\"\"\n",
        "    self.suggestion = None\n",
        "    \n",
        "    self.me = Me()\n",
        "    self.you = You()\n",
        "    self.parties  = [ self.me, self.you ]\n",
        "    \n",
        "    self.conversation = []\n",
        "    \n",
        "    lines = prior.split(\"\\n\")\n",
        "    for line in lines:\n",
        "      line = line.strip()\n",
        "      if len(line)!=0:\n",
        "        party = None\n",
        "        for party in self.parties:\n",
        "          if party.matches(line):\n",
        "            break\n",
        "        if party is None:\n",
        "          raise Exception(f\"Unknown party: {line}\")\n",
        "                \n",
        "        self.conversation.append((party,line))\n",
        "    self.get_suggestion()\n",
        "    \n",
        "  \n",
        "  def get_prior(self):\n",
        "    conv = \"\"\n",
        "    for (party, line) in self.conversation:\n",
        "      conv+=line+\"\\n\"\n",
        "    return conv\n",
        "  \n",
        "  def get_suggestion(self):\n",
        "    who, last_line = self.conversation[-1]\n",
        "\n",
        "    party_index = self.parties.index(who)\n",
        "    next_party = self.parties[(party_index+1) % len(self.parties)]\n",
        "      \n",
        "    conv = self.get_prior()\n",
        "    conv += next_party.get_random_prefix()\n",
        "    answer = self.get_answer(next_party, conv)\n",
        "\n",
        "    if not next_party.matches(answer):\n",
        "      prefix = next_party.get_random_prefix()\n",
        "      answer = prefix + answer\n",
        "    \n",
        "    self.suggestion = (next_party, answer)\n",
        "  \n",
        "  def next(self, party = None, answer = \"\"):\n",
        "    \"\"\"Continue the conversation\n",
        "    :param party: None -> use the current party which is currently in turn\n",
        "    :param answer: None -> use the suggestion, specify a text to override the \n",
        "           suggestion\n",
        "    \n",
        "    \"\"\"\n",
        "    suggested_party, suggested_answer = self.suggestion\n",
        "    if party is None:\n",
        "      party = suggested_party\n",
        "    \n",
        "    if answer == \"\":\n",
        "      answer = suggested_answer\n",
        "      \n",
        "    if not party.matches(answer):\n",
        "      prefix = party.get_random_prefix()\n",
        "      answer = prefix + answer\n",
        "    \n",
        "    answer = answer.strip()\n",
        "    if answer[-1] != \"\\\"\":\n",
        "      # add the closing \"\n",
        "      answer += \"\\\"\"\n",
        "      \n",
        "    self.conversation.append((party, answer))    \n",
        "    self.get_suggestion()\n",
        "    \n",
        "  def retry(self):\n",
        "    self.get_suggestion()\n",
        "        \n",
        "  def get_answer(self, party, conv):\n",
        "    answer = gpt2.generate_conditional(raw_text=conv)\n",
        "    lines = answer.split(\"\\n\")\n",
        "    line = \"\"\n",
        "    for line in lines:\n",
        "      if line !=\"\":\n",
        "        break\n",
        "      \n",
        "    if line!=\"\":\n",
        "      return line\n",
        "    \n",
        "    return \"\"\n",
        "      \n",
        "  def show(self):\n",
        "    conv = \"\"\n",
        "    for (party, line) in self.conversation:\n",
        "      conv+=line+\"\\n\"\n",
        "    print(conv)\n",
        "    if self.suggestion is not None:\n",
        "      party, answer  = self.suggestion\n",
        "      print(\"--> \"+answer)\n",
        "    \n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GYnMZDPgr7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c = Conversation()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taqx6zfNVbru",
        "colab_type": "code",
        "outputId": "03ed2a10-1362-45b5-92bd-b10da9b30b03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# show the conversation and the suggestion by the ai\n",
        "c.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You said: \"Nice to meet you. What's your name?\"\n",
            "I said: \"My name is Pete.\"\n",
            "You said: \"That's an interesting name. How old are you?\"\n",
            "I said: \"I'm 40 years old.\"\n",
            "You said: \"Can you tell me something about yourself?\"\n",
            "I said: \"Ofcourse! I like playing video games and eating cake. \"\n",
            "You said: \"I like sweet stuff too. What are your plans for tomorrow?\"\n",
            "\n",
            "--> I said: \"Once I get to the park today and walk with my friends, I'm going to shave my coif with tooth paste.\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUICPSgdfcp_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \"I said\" -> answer by the AI\n",
        "# if the answer of the AI is garbage then call c.retry() \n",
        "c.retry()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SvC-10-WTKH",
        "colab_type": "code",
        "outputId": "4b9babc6-5043-49e8-89e3-c92249e57d81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "c.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You said: \"Nice to meet you. What's your name?\"\n",
            "I said: \"My name is Pete.\"\n",
            "You said: \"That's an interesting name. How old are you?\"\n",
            "I said: \"I'm 40 years old.\"\n",
            "You said: \"Can you tell me something about yourself?\"\n",
            "I said: \"Ofcourse! I like playing video games and eating cake. \"\n",
            "You said: \"I like sweet stuff too. What are your plans for tomorrow?\"\n",
            "\n",
            "--> I said: \"I might go to the zoo in London.\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNsuyeKyeNeq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Accept the suggested andwer by the AI\n",
        "c.next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ8F3HbyUDZh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# now its your turn\n",
        "c.next(c.you, \"My favorite pizza is the calzone with lots of cheese.\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ts3yigFeQ-8",
        "colab_type": "code",
        "outputId": "062e5c0b-6815-4cd7-b060-e8ddb348ce7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "c.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You said: \"Nice to meet you. What's your name?\"\n",
            "I said: \"My name is Pete.\"\n",
            "You said: \"That's an interesting name. How old are you?\"\n",
            "I said: \"I'm 40 years old.\"\n",
            "You said: \"Can you tell me something about yourself?\"\n",
            "I said: \"Ofcourse! I like playing video games and eating cake. \"\n",
            "You said: \"I like sweet stuff too. What are your plans for tomorrow?\"\n",
            "I said: \"I might go to the zoo in London.\"\n",
            "You said: \"My favorite pizza is the calzone with lots of cheese.\"\n",
            "\n",
            "--> I said: \"Oh! Then I'll have some of that!\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR5UP1WQXCi0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c.next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUiQqYdEhmwe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now its your turn\n",
        "c.next(c.you, \"You eat pizza often?\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snnqjfilc1Ai",
        "colab_type": "code",
        "outputId": "5486ec9a-c057-485a-b565-d60b37d2ba0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# show the conversation and the reply of the ai\n",
        "c.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You said: \"Nice to meet you. What's your name?\"\n",
            "I said: \"My name is Pete.\"\n",
            "You said: \"That's an interesting name. How old are you?\"\n",
            "I said: \"I'm 40 years old.\"\n",
            "You said: \"Can you tell me something about yourself?\"\n",
            "I said: \"Ofcourse! I like playing video games and eating cake. \"\n",
            "You said: \"I like sweet stuff too. What are your plans for tomorrow?\"\n",
            "I said: \"I might go to the zoo in London.\"\n",
            "You said: \"My favorite pizza is the calzone with lots of cheese.\"\n",
            "You said: \"You eat pizza often?\"\n",
            "You said: \"You eat pizza often?\"\n",
            "\n",
            "--> I said: \"Yeah!\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8R6da2YMS6fD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# accept ai answer\n",
        "c.next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPJZ_aJxltCm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# our reply\n",
        "c.next(c.you, \"Pizza is not to good for your health though.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e66v5ew6sQW4",
        "colab_type": "code",
        "outputId": "1d9a6f58-15f9-4180-a380-2af044d5a31a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# show the conv. and reply of the ai\n",
        "c.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You said: \"Nice to meet you. What's your name?\"\n",
            "I said: \"My name is Pete.\"\n",
            "You said: \"That's an interesting name. How old are you?\"\n",
            "I said: \"I'm 40 years old.\"\n",
            "You said: \"Can you tell me something about yourself?\"\n",
            "I said: \"Ofcourse! I like playing video games and eating cake. \"\n",
            "You said: \"I like sweet stuff too. What are your plans for tomorrow?\"\n",
            "I said: \"I might go to the zoo in London.\"\n",
            "You said: \"My favorite pizza is the calzone with lots of cheese.\"\n",
            "You said: \"You eat pizza often?\"\n",
            "You said: \"You eat pizza often?\"\n",
            "You said: \"Pizza is not to good for your health though.\"\n",
            "\n",
            "--> I said: \"I read that in this place colas can be brought in. Do you like Coke?\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHjyqH6xoMr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}